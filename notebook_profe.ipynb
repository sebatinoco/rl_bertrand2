{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: dqn_experiment \t Episode: 1/1 \t Episode completion: 100.00 % \t Delta: 0.69"
     ]
    }
   ],
   "source": [
    "from agents.dqn import DQNAgent\n",
    "from agents.sac import SACAgent\n",
    "from envs.LinearBertrandInflation import LinearBertrandEnv\n",
    "from envs.BertrandInflation import BertrandEnv\n",
    "from replay_buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "N = 2\n",
    "k = 1\n",
    "rho = 1e-3\n",
    "timesteps = 250_000\n",
    "buffer_size = 200_000\n",
    "sample_size = 256\n",
    "update_steps = 1\n",
    "deviate_start = 0.8\n",
    "deviate_end = 0.9\n",
    "random_state = 3381\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "variation = 'base'\n",
    "\n",
    "dim_states = (N * k) + k + 1\n",
    "dim_actions = 15\n",
    "#dim_actions = 1\n",
    "\n",
    "#env = LinearBertrandEnv(N, k, rho, timesteps, dim_actions=dim_actions)\n",
    "env = BertrandEnv(N, k, rho, timesteps, dim_actions=dim_actions, max_var=2.0, mu = 0.25,\n",
    "                  random_state = random_state, use_inflation_data=True, normalize=False)\n",
    "buffer = ReplayBuffer(dim_states, N, buffer_size, sample_size)\n",
    "\n",
    "agents = [DQNAgent(dim_states, dim_actions, beta = 5e-5, lr = 1e-1, random_state = random_state + _) for _ in range(N)]\n",
    "#agents = [SACAgent(dim_states, dim_actions) for _ in range(N)]\n",
    "\n",
    "exp_name = 'dqn_experiment'\n",
    "episodes = 1\n",
    "\n",
    "prices_history = np.zeros((episodes, timesteps, N))\n",
    "actions_history = np.zeros((episodes, timesteps, N))\n",
    "costs_history = np.zeros((episodes, timesteps))\n",
    "monopoly_history = np.zeros((episodes, timesteps))\n",
    "nash_history = np.zeros((episodes, timesteps))\n",
    "rewards_history = np.zeros((episodes, timesteps, N))\n",
    "delta_history = np.zeros((episodes, timesteps))\n",
    "quantities_history = np.zeros((episodes, timesteps, N))\n",
    "pi_N_history = np.zeros((episodes, timesteps))\n",
    "pi_M_history = np.zeros((episodes, timesteps))\n",
    "A_history = np.zeros((episodes, timesteps))\n",
    "\n",
    "ob_t = env.reset()\n",
    "for episode in range(episodes):\n",
    "    for t in range(timesteps):\n",
    "        \n",
    "        actions = [agent.select_action(ob_t) for agent in agents]\n",
    "        \n",
    "        if variation == 'deviate':\n",
    "            if (t/timesteps > deviate_start) and (t/timesteps <= deviate_end):\n",
    "                env.trigger_deviation = True\n",
    "            \n",
    "            elif t/timesteps > deviate_end:\n",
    "                env.trigger_deviation = False\n",
    "        \n",
    "        elif variation == 'altruist':\n",
    "            env.altruist = True\n",
    "        \n",
    "        ob_t1, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        experience = (ob_t, actions, rewards, ob_t1, done)\n",
    "        \n",
    "        buffer.store_transition(*experience)\n",
    "        \n",
    "        if (t % update_steps == 0) & (t >= buffer.sample_size):\n",
    "            for agent_idx in range(N):\n",
    "                agent = agents[agent_idx]\n",
    "                sample = buffer.sample(agent_idx)\n",
    "                agent.update(*sample)\n",
    "                \n",
    "        sys.stdout.write(f\"\\rExperiment: {exp_name} \\t Episode: {episode + 1}/{episodes} \\t Episode completion: {100 * t/timesteps:.2f} % \\t Delta: {info:.2f}\")\n",
    "                \n",
    "        ob_t = ob_t1\n",
    "        \n",
    "    # store episode metrics\n",
    "    prices_history[episode] = np.array(env.prices_history)[-timesteps:]\n",
    "    actions_history[episode] = np.array(env.action_history)[-timesteps:]\n",
    "    costs_history[episode] = np.array(env.costs_history)[-timesteps:]\n",
    "    monopoly_history[episode] = np.array(env.monopoly_history)[-timesteps:]\n",
    "    nash_history[episode] = np.array(env.nash_history)[-timesteps:]\n",
    "    rewards_history[episode] = np.array(env.rewards_history)[-timesteps:]\n",
    "    delta_history[episode] = np.array(env.metric_history)[-timesteps:]\n",
    "    quantities_history[episode] = np.array(env.quantities_history)[-timesteps:]\n",
    "    pi_N_history[episode] = np.array(env.pi_N_history)[-timesteps:]\n",
    "    pi_M_history[episode] = np.array(env.pi_M_history)[-timesteps:]\n",
    "    A_history[episode] = np.array(env.A_history)[-timesteps:]\n",
    "\n",
    "prices_history = np.mean(prices_history, axis = 0)\n",
    "actions_history = np.mean(actions_history, axis = 0)\n",
    "costs_history = np.mean(costs_history, axis = 0)\n",
    "monopoly_history = np.mean(monopoly_history, axis = 0)\n",
    "nash_history = np.mean(nash_history, axis = 0)\n",
    "rewards_history = np.mean(rewards_history, axis = 0)\n",
    "delta_history = np.mean(delta_history, axis = 0)\n",
    "quantities_history = np.mean(quantities_history, axis = 0)\n",
    "pi_N_history = np.mean(pi_N_history, axis = 0)\n",
    "pi_M_history = np.mean(pi_M_history, axis = 0)\n",
    "A_history = np.mean(A_history, axis = 0) # equal disposition to pay\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('models/agent_3381.pkl', 'wb') as file:\n",
    "    pickle.dump(agents[0], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: dqn_experiment \t Episode: 1/1 \t Episode completion: 100.00 % \t Delta: 0.77"
     ]
    }
   ],
   "source": [
    "from agents.dqn import DQNAgent\n",
    "from agents.sac import SACAgent\n",
    "from envs.LinearBertrandInflation import LinearBertrandEnv\n",
    "from envs.BertrandInflation import BertrandEnv\n",
    "from replay_buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "N = 2\n",
    "k = 1\n",
    "rho = 1e-3\n",
    "timesteps = 250_000\n",
    "buffer_size = 200_000\n",
    "sample_size = 256\n",
    "update_steps = 1\n",
    "deviate_start = 0.8\n",
    "deviate_end = 0.9\n",
    "random_state = 3382\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "variation = 'base'\n",
    "\n",
    "dim_states = (N * k) + k + 1\n",
    "dim_actions = 15\n",
    "#dim_actions = 1\n",
    "\n",
    "#env = LinearBertrandEnv(N, k, rho, timesteps, dim_actions=dim_actions)\n",
    "env = BertrandEnv(N, k, rho, timesteps, dim_actions=dim_actions, max_var=2.0, mu = 0.25,\n",
    "                  random_state = random_state, use_inflation_data=True, normalize=False)\n",
    "buffer = ReplayBuffer(dim_states, N, buffer_size, sample_size)\n",
    "\n",
    "agents = [DQNAgent(dim_states, dim_actions, beta = 5e-5, lr = 1e-1, random_state = random_state + _) for _ in range(N)]\n",
    "#agents = [SACAgent(dim_states, dim_actions) for _ in range(N)]\n",
    "\n",
    "exp_name = 'dqn_experiment'\n",
    "episodes = 1\n",
    "\n",
    "prices_history = np.zeros((episodes, timesteps, N))\n",
    "actions_history = np.zeros((episodes, timesteps, N))\n",
    "costs_history = np.zeros((episodes, timesteps))\n",
    "monopoly_history = np.zeros((episodes, timesteps))\n",
    "nash_history = np.zeros((episodes, timesteps))\n",
    "rewards_history = np.zeros((episodes, timesteps, N))\n",
    "delta_history = np.zeros((episodes, timesteps))\n",
    "quantities_history = np.zeros((episodes, timesteps, N))\n",
    "pi_N_history = np.zeros((episodes, timesteps))\n",
    "pi_M_history = np.zeros((episodes, timesteps))\n",
    "A_history = np.zeros((episodes, timesteps))\n",
    "\n",
    "ob_t = env.reset()\n",
    "for episode in range(episodes):\n",
    "    for t in range(timesteps):\n",
    "        \n",
    "        actions = [agent.select_action(ob_t) for agent in agents]\n",
    "        \n",
    "        if variation == 'deviate':\n",
    "            if (t/timesteps > deviate_start) and (t/timesteps <= deviate_end):\n",
    "                env.trigger_deviation = True\n",
    "            \n",
    "            elif t/timesteps > deviate_end:\n",
    "                env.trigger_deviation = False\n",
    "        \n",
    "        elif variation == 'altruist':\n",
    "            env.altruist = True\n",
    "        \n",
    "        ob_t1, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        experience = (ob_t, actions, rewards, ob_t1, done)\n",
    "        \n",
    "        buffer.store_transition(*experience)\n",
    "        \n",
    "        if (t % update_steps == 0) & (t >= buffer.sample_size):\n",
    "            for agent_idx in range(N):\n",
    "                agent = agents[agent_idx]\n",
    "                sample = buffer.sample(agent_idx)\n",
    "                agent.update(*sample)\n",
    "                \n",
    "        sys.stdout.write(f\"\\rExperiment: {exp_name} \\t Episode: {episode + 1}/{episodes} \\t Episode completion: {100 * t/timesteps:.2f} % \\t Delta: {info:.2f}\")\n",
    "                \n",
    "        ob_t = ob_t1\n",
    "        \n",
    "    # store episode metrics\n",
    "    prices_history[episode] = np.array(env.prices_history)[-timesteps:]\n",
    "    actions_history[episode] = np.array(env.action_history)[-timesteps:]\n",
    "    costs_history[episode] = np.array(env.costs_history)[-timesteps:]\n",
    "    monopoly_history[episode] = np.array(env.monopoly_history)[-timesteps:]\n",
    "    nash_history[episode] = np.array(env.nash_history)[-timesteps:]\n",
    "    rewards_history[episode] = np.array(env.rewards_history)[-timesteps:]\n",
    "    delta_history[episode] = np.array(env.metric_history)[-timesteps:]\n",
    "    quantities_history[episode] = np.array(env.quantities_history)[-timesteps:]\n",
    "    pi_N_history[episode] = np.array(env.pi_N_history)[-timesteps:]\n",
    "    pi_M_history[episode] = np.array(env.pi_M_history)[-timesteps:]\n",
    "    A_history[episode] = np.array(env.A_history)[-timesteps:]\n",
    "\n",
    "prices_history = np.mean(prices_history, axis = 0)\n",
    "actions_history = np.mean(actions_history, axis = 0)\n",
    "costs_history = np.mean(costs_history, axis = 0)\n",
    "monopoly_history = np.mean(monopoly_history, axis = 0)\n",
    "nash_history = np.mean(nash_history, axis = 0)\n",
    "rewards_history = np.mean(rewards_history, axis = 0)\n",
    "delta_history = np.mean(delta_history, axis = 0)\n",
    "quantities_history = np.mean(quantities_history, axis = 0)\n",
    "pi_N_history = np.mean(pi_N_history, axis = 0)\n",
    "pi_M_history = np.mean(pi_M_history, axis = 0)\n",
    "A_history = np.mean(A_history, axis = 0) # equal disposition to pay\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('models/agent_3382.pkl', 'wb') as file:\n",
    "    pickle.dump(agents[0], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESPERAR A ENTRENAR AGENTES 3381 Y 3382\n",
    "# CARGAR AGENTES EN AMBIENTE TRAIN TEST CON SEMILLA 500\n",
    "# GRAFICAR\n",
    "\n",
    "# LUEGO EXPORTAR RESULTADOS A LATEX\n",
    "# GENERAR FIGURAS Y REALIZAR PRESENTACION --> DEBERIA AVANZAR EN ESTOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_metrics import get_rolling, get_rolling_std\n",
    "import pandas as pd\n",
    "\n",
    "## EXPORT\n",
    "results = pd.DataFrame({'costs': costs_history,\n",
    "                        'pi_N': pi_N_history,\n",
    "                        'pi_M': pi_M_history,\n",
    "                        'delta': delta_history,\n",
    "                        'p_nash': nash_history,\n",
    "                        'p_monopoly': monopoly_history,\n",
    "                        'A': A_history,\n",
    "                        })\n",
    "\n",
    "for agent in range(env.N):\n",
    "    results[f'actions_{agent}'] = actions_history[:, agent]\n",
    "    results[f'prices_{agent}'] = prices_history[:, agent]\n",
    "    results[f'quantities_{agent}'] = quantities_history[:, agent]\n",
    "    results[f'rewards_{agent}'] = rewards_history[:, agent]\n",
    "    \n",
    "results.to_csv(f'test2.csv', index = False, sep = ';', encoding = 'utf-8-sig')\n",
    "\n",
    "## READ AND PREPARE\n",
    "window_size = 1000\n",
    "df_avg = pd.DataFrame()\n",
    "df_std = pd.DataFrame()\n",
    "\n",
    "df_plot = pd.read_csv('test2.csv', sep = ';', encoding = 'utf-8-sig')\n",
    "\n",
    "actions_cols = [col for col in df_plot.columns if 'actions' in col]\n",
    "price_cols = [col for col in df_plot.columns if 'prices' in col]\n",
    "rewards_cols = [col for col in df_plot.columns if 'rewards' in col]\n",
    "quantities_cols = [col for col in df_plot.columns if 'quantities' in col]\n",
    "\n",
    "n_agents = len(actions_cols)\n",
    "\n",
    "df_plot['avg_actions'] = df_plot[actions_cols].mean(axis = 1)\n",
    "df_plot['avg_prices'] = df_plot[price_cols].mean(axis = 1)\n",
    "df_plot['avg_rewards'] = df_plot[rewards_cols].mean(axis = 1)\n",
    "df_plot['avg_quantities'] = df_plot[quantities_cols].mean(axis = 1)\n",
    "avg_cols = [col for col in df_plot.columns if 'avg' in col]\n",
    "\n",
    "window_cols = price_cols + rewards_cols + quantities_cols + avg_cols + ['delta']\n",
    "for col in window_cols:\n",
    "    df_avg[col] = get_rolling(df_plot[col], window_size = window_size)\n",
    "    df_std[col] = get_rolling_std(df_plot[col], window_size = window_size)\n",
    "\n",
    "series_size = df_avg.shape[0]\n",
    "\n",
    "df_plot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "for agent in range(n_agents):\n",
    "    serie = f'prices_{agent}'\n",
    "    #plt.plot(price_serie, label = f'Agent {agent}')\n",
    "    plt.errorbar(range(series_size), df_avg[serie], df_std[serie], errorevery=int(0.01 * series_size), label = f'Agent {agent}')\n",
    "plt.plot(df_plot['p_monopoly'], color = 'red', label = 'Monopoly price')\n",
    "plt.plot(df_plot['p_nash'], color = 'green', label = 'Nash price')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Prices')\n",
    "plt.title('Experiments Results Sample')\n",
    "plt.legend()\n",
    "plt.savefig('prices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "std_avg = get_rolling(agents[0].mean_history, window_size)\n",
    "std_std = get_rolling_std(agents[0].mean_history, window_size)\n",
    "\n",
    "std_size = len(std_std)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.errorbar(range(std_size), std_avg, std_std, errorevery=int(0.01 * std_size))\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Standard Deviation (std)')\n",
    "plt.title('Standard Deviation of Agent 0')\n",
    "#plt.savefig('desviacion_estandar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "std_avg = get_rolling(agents[0].std_history, window_size)\n",
    "std_std = get_rolling_std(agents[0].std_history, window_size)\n",
    "\n",
    "std_size = len(std_std)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.errorbar(range(std_size), std_avg, std_std, errorevery=int(0.01 * std_size))\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Standard Deviation (std)')\n",
    "plt.title('Standard Deviation of Agent 0')\n",
    "#plt.savefig('desviacion_estandar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size = len(df_avg[serie])\n",
    "plt.figure(figsize = (12, 4))\n",
    "#plt.plot(df_plot['avg_prices'], label = 'Average prices')\n",
    "plt.errorbar(range(series_size), df_avg['avg_prices'], df_std['avg_prices'], errorevery=int(0.01 * series_size), label = f'Average prices')\n",
    "plt.plot(df_plot['p_monopoly'], color = 'red', label = 'Monopoly price')\n",
    "plt.plot(df_plot['p_nash'], color = 'green', label = 'Nash price')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Prices')\n",
    "plt.legend()\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "#plt.plot(df_plot['avg_rewards'], label = 'Average profits')\n",
    "plt.errorbar(range(series_size), df_avg['avg_rewards'], df_std['avg_rewards'], errorevery=int(0.01 * series_size), label = f'Average profits')\n",
    "plt.plot(df_plot['pi_N'], label = 'Nash profits', color = 'green')\n",
    "plt.plot(df_plot['pi_M'], label = 'Monopoly profits', color = 'red')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Profits')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "#plt.plot(df_plot['delta'], label = 'Average profits')\n",
    "plt.errorbar(range(series_size), df_avg['delta'], df_std['delta'], errorevery=int(0.01 * series_size), label = f'Average profits')\n",
    "plt.axhline(1, color = 'red', label = 'Nash profits')\n",
    "plt.axhline(0, color = 'green', label = 'Monoply profits')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Delta')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "df  = pd.read_csv('test2.csv', sep = ';')\n",
    "df = df.iloc[8000:8000 + sample_size]\n",
    "df['rewards_0'] = df['rewards_0'].replace(0, np.nan)\n",
    "df['rewards_1'] = df['rewards_1'].replace(0, np.nan)\n",
    "\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crea un histograma para la columna\n",
    "plt.hist(df['prices_0'], bins=30, color='blue', alpha=0.7)\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Precios')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crea un histograma para la columna\n",
    "plt.hist(df['rewards_0'], bins=30, color='blue', alpha=0.7)\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Recompensas')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
